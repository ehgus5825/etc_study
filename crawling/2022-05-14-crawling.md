# crawling 외주 업무 / 내용

intro
--------

대학교 연구실에서 일을 하고 있는 친구에게 크롤링 업무를 해보겠냐고 제시가 들어와서 아르바이트로 크롤링을 맡게 되었다. 처음이고 잘 모르더라도 크게 어렵지 않을 것이라는 말에 승락하게 되었다.

크롤링할 내용은 국민신문고와 국민청원의 데이터였다. 국민신문고는 친구가 이전에 크롤링했던 코드를 받았고 국민청원의 코드는 직접 짜야했다. 시기가 정보처리기사 시험과 겹쳐서 두마리 토끼를 다 잡으려면 빠르게 했어야했지만 나는 역시 게을렀고 이 업무가 쉽다고 해서 조금 낮잡아 보았다.

2주 가량의 시간이 주어졌고 나는 낮잡아 봤기 때문에 정처기 공부를 잠시 중단하고 일주일 정도 전에 시작했다. 하지만 크롤링 해야하는 데이터는 생각보다 많았고 크롤링 중에 발생하는 에러 때문에 실행을 중단하고 코드를 고치고 해서 시간이 넉넉하지 않았다. 그래도 이 기회로 크롤링에 대한 기술도 습득할 수 있을 것이란 생각에 진득하게 코드 해석도 하고 공부도 하였다. (~~하지만 정처기는 떨어졌다.~~)

크롤링에 사용된 언어는 python이고 구글의 webdriver과 selenium을 사용해서 데이터를 수집했다.

Crawling
-------

크롤링의 핵심은 webdriver을 통해 웹페이지를 제어하는 것이 전부였다. webdriver을 통해서 제어하는 방법은 dom이나 jQuery의 메서드를 사용하는 것과 비슷했고 그렇기 때문에 이전에 웹공부를 조금 해놓은 것이 도움이 조금 된 것 같다. webdriver의 제어권을 driver 변수에 담아서 driver를 로드하고 driver을 통해서 웹페이지를 제어하며 데이터를 크롤링한다. 아래의 코드는 driver를 로드하고 url을 입력하여 해당 url의 창을 띄우는 코드이다.

``` python
# webdriver 켜기
from selenium import webdriver
from selenium.webdriver.chrome.service import Service

# webdriver 켜기
path = Service('C:/Users/dohyun/Downloads/chromedriver_win32/chromedriver.exe')
driver = webdriver.Chrome(service=path)

# 크롬창 띄우기
driver.get('접근할 URL') # URL을 보고 크롬에서 인터넷 창을 띄운다.

```

정말 크롤링 업무가 크게 어렵지 않다는 말이 맞는게 대부분의 코드가 거의 다 아래와 같은 형식으로 되어 있었다. 코드를 설명하자면 전체 driver(document) 내에서 'XPATH'에 해당하는 태그(엘리먼트)를 최대 99999초 까지 기다려서 찾고 그 후 그 태그를 click() 하겠다는 말이다.

``` python
WebDriverWait(driver, 99999).until(     
    EC.element_to_be_clickable((            # or EC.presence_of_all_elements_located() or EC.presence_of_element_located()
        By.XPATH, 'XPATH'))).click()        # or text()
        # 형식이 XPATH       # 그후 click()

```
또 EC의 메소드로 presence_of_all_elements_located() 나 presence_of_element_located()를 사용할 수 있다. 첫번째 메소드는 해당되는 모든 태그를 반환하기 때문에 태그들의 유사배열을 반환하고 두번째 메소드는 찾을 수 있는 태그 중 첫번째 태그만 반환하는 메소드이다.

따라서 위의 방법들로 원하는 태그를 반환받아서 click()이나 text() 메소드를 사용하여 페이지 탐색 혹은 크롤링을 할 수 있다.

아래의 코드는 webdriver을 조작하여 원하는 내용까지 탐색하거나 원하는 내용을 크롤링하는 코드이다. 

``` python
# webdriver 조작에 쓰이는 라이브러리
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# driver 내에서 'XPATH'에 부합하는 태그를 찾아서 click()
WebDriverWait(driver, 99999).until(
    EC.element_to_be_clickable((
        By.XPATH, 'XPATH'))).click()

# driver 내에서 'XPATH'에 부합하는 모든 태그를 찾고 그 중 첫번째 태그의 text를 data1에 담음
data1 = WebDriverWait(driver, 99999).until(
    EC.presence_of_all_elements_located((
        By.XPATH, 'XPATH')))[0].text()

# driver 내에서 'XPATH'에 부합하는 태그를 찾아 그 text를 data2에 담음
data2 = WebDriverWait(driver, 99999).until(
    EC.presence_of_element_located((
        By.XPATH, 'XPATH'))).text()

# 데이터를 배열의 원소로 하여 종합함
data = [data1, data2]

# drive에서 XPATH에 해당하는 엘리먼트가 지금 화면에 표시되어 있는지 확인
# 필자는 화면에서 '상세정보 열기'와 같은 토글로 되어있는 엘리먼트 들에 사용 try..except와 함께 사용 (false시 오류가 발생하기 때문)
driver.find_element(By.XPATH, 'XPATH').is_displayed()
driver.find_elements(By.XPATH, 'XPATH').is_displayed()

# 뒤로가기 구현
driver.back()
```

위의 코드는 정말 간단한 사용법이며 예시이다. 코드를 짜는 것은 개인의 역량이며 페이지 마다 코드는 다 틀려지며 무궁무진하게 활용이 될것이라고 생각한다. 코드를 짜는데 더 필요한 정보는 정규식이나 파이썬 문법 내용이기 때문에 본 게시글에서는 생략하도록 한다.

또 한가지 꿀팁이 있다면 웹페이지 마다 로드 속도가 다 다르다. 따라서 웹페이지 로드 속도보다 driver의 제어 속도가 더 빠르게 된다면 로드보다 코드가 먼저 실행되면서 먹통이 되어 크롤링이 중단된다. 따라서 그러한 문제가 발생하는 부분에 'time.sleep(1)' 과 같은 코드를 사용해서 코드 실행을 잠시 멈추면 로드에 따른 문제를 어느정도는 해결할 수 있다.

``` python
from time 

time.sleep(1)
```

크롤링한 데이터들을 csv 파일로 변환하여 생성하였는데 그 방법은 아래와 같다.

``` python
import csv   # csv 파일 쓰기 메소드 writer 사용해서 csv 파일 수정

# str(Path('main.py').absolute()) main.py 의 절대 경로를 str 로 만듬
# split('\\')[:-1] 을 사용해서 '\' 로 자르고 리스트로 반환  
path = str(Path('main.py').absolute()).split('\\')[:-1]
file = open(f'{"/".join(path)}/data/4_{self.keyword}.csv',
            'w', encoding='utf-8-sig', newline='')
# path 경로를 기반으로 csv 파일 생성 후 쓰기 권한으로 열기

# csv file 쓰기 권한 받기
writer = csv.writer(file)
# 첫줄 데이터 제목 삽입 (EX. 제목, 날짜)
writer.writerow(['제목','날짜'])
# 전체 데이터 한번에 삽입
writer.writerows(data)
```

자료 통합 및 중복 제거
-------

앞에서 검색 키워드 별로 크롤링 하여 자료를 수집을 했다.
그런데 내 업무는 데이터를 전부 합치고 중복 제거까지 하는 것이 나의 업무였다.

자료 통합은 어렵지 않았다. cmd에서 파일이 있는 디렉토리까지 들어간 후 "type * > allfile.csv" 명령어를 입력하면 전체 파일이 통합된다. 

``` 
    C:\Users\dohyun\Desktop\crawling> type * > allfile.csv
```

하지만 중복제거가 쉽지 않았다. 중복제거는 pandas라는 파이썬 라이브러리를 사용하였는데 문제가 발생하지 않았다면 원래의 코드는 간단했다. 

pandas에는 csv 파일를 읽는 read_csv() 메소드와 csv 파일로 변환하는 to_csv() 메소드, 데이터를 표 형식으로 만들어주는 DataFrame(), 그 데이터를 중복제거하는 drop_duplicates() 메소드를 제공한다. 따라서 앞의 메소드들을 사용하면 편리하게 중복제거를 할 수 있지만 내가 크롤링한 csv파일은 인코딩 문제 때문에 read_csv() 메소드를 사용할때 에러가 발생했다.

``` python
# 기존의 코드
import pandas as pd

csv_list = pd.read_csv('C:/Users/dohyun/Desktop/crawling/data/완/1_공개제안/all_file1.csv',  encoding='utf-8-sig')
                                                # csv 파일 읽어서 데이터 형식에 맡게 알아서 저장
df = pd.DataFrame(csv_list)                     # 데이터 표형식으로 변환
tmp = df.drop_duplicates(['글제목', '신청일'])   # 글제목 && 신청일 칼럼이 같은 데이터들 중복제거
tmp.to_csv("1_통합.csv")                        # 데이터 csv 파일로 변환
```

따라서 다른 방법으로 파일을 열고 읽어야 했다. 

``` python
# 수정된 코드
import pandas as pd
import csv

csv.field_size_limit(100000000)
f = open('C:/Users/dohyun/Desktop/crawling/data/완/1_공개제안/all_file1.csv',
         encoding='utf-8')
reader = csv.reader(f)
csv_list = []
for l in reader:
    csv_list.append(l)
f.close()                                           # 파일 열고 한 행씩 배열에 추가

df = pd.DataFrame(csv_list, columns=[
    '검색 키워드', '검색 범위', '글제목', '평점', '분야', '처리기관', '현황및문제점', '개선방안', '기대효과', '신청일', '첨부파일'
])                                                  # 데이터 표형식으로 변환 (칼럼명 직접 추가)
tmp = df.drop_duplicates(['글제목', '신청일'])       # 글제목 && 신청일 칼럼이 같은 데이터들 중복제거
tmp.to_csv("1_통합.csv")                            # 데이터 csv 파일로 변환
```

파이썬에 내장된 메소드(open)로 파일을 열고 csv의 reader로 파일을 읽어 그 파일들을 반복문을 돌려 csv_list라는 배열에 직접 추가해는 번거로움을 거쳐서 중복제거를 할 수 있었다. 그런데 이 마저도 오류가 발생했다. csv의 파일이 너무 커서 범위를 초과하는 문제가 발생했고 csv의 field_size_limit의 범위를 재설정하여 해결할 수 있었다. 

후기
------

업무를 하면서 '이게 맞나'라는 생각을 좀 했던 것 같다. 당시 나는 취업준비를 하면서 많이 위축되었고 뭐든 저지르고 바쁘게 살아보자는 생각에 승낙하였는데 결과적으로는 재미는 있었지만 정보처리기사 시험을 망쳤기 때문에 좀 씁쓸한 건 사실이다. 

업무를 받는 날로 돌아간다면 바로바로 일을 처리했어야 했다. 업무가 쉽긴하지만 조금 노가다적인 일이며 크롤링이 잘되는지 수시로 확인도 해줘야하고 멈추면 코드도 고쳐야하고 여간 번거로운 일이 아니였다. 좋은 경험이였지만 다시 한다면 좀 더 부지런하게 해야겠다는 생각을 하며 반성하고 있다.
